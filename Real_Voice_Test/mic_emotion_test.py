# ìŒì„± ë°ì´í„° ì…‹ì´ ì•„ë‹Œ, ì‹¤ì œ ëª©ì†Œë¦¬ë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸ í•˜ëŠ” ì½”ë“œ
import numpy as np
import torch, torch.nn as nn
import librosa, pyaudio

# ====== í•™ìŠµ ìŠ¤í™ ======
MODEL_PATH  = r"C:\github\System\Voice_Emotion_classification\model\cnn_gru.pt"
TARGET_SR   = 16000
N_MFCC      = 40
N_FFT       = 2048          # librosa ê¸°ë³¸(í•™ìŠµì—ì„œë„ ê¸°ë³¸ ì‚¬ìš©)
HOP_LENGTH  = 512           # librosa ê¸°ë³¸(í•™ìŠµì—ì„œë„ ê¸°ë³¸ ì‚¬ìš©)
MAX_T       = 128           # train.MFCCDataset(max_len)

EMOTIONS = ['anger','disgust','fear','happiness','neutral','sadness','surprise']
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ====== ëª¨ë¸ êµ¬ì¡° (train.pyì™€ ë™ì¼) ======
class CNN_GRU_Model(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d((1, 2))  # [B, 64, 40, T/2]
        )
        self.gru = nn.GRU(input_size=64, hidden_size=128, num_layers=2,
                          batch_first=True, bidirectional=True, dropout=0.3)
        self.fc = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(128 * 2, num_classes)
        )
    def forward(self, x):                 # x: [B,1,40,T]
        x = self.cnn(x)                   # [B,64,40,T/2]
        x = x.mean(dim=2)                 # [B,64,T/2]
        x = x.permute(0,2,1)              # [B,T/2,64]
        out,_ = self.gru(x)               # [B,T/2,256]
        out = out[:, -1, :]
        return self.fc(out)

# ====== ì „ì²˜ë¦¬ í•¨ìˆ˜ (extract_mfcc.pyì™€ ë™ì¼ ë¡œì§) ======
def make_mfcc(y, sr):
    # í•„ìš”ì‹œ ë¦¬ìƒ˜í”Œ
    if sr != TARGET_SR:
        y = librosa.resample(y=y, orig_sr=sr, target_sr=TARGET_SR, res_type="kaiser_fast")
        sr = TARGET_SR
    # float32 -1~1
    y = y.astype(np.float32)
    if np.max(np.abs(y)) > 0: y = y / (np.max(np.abs(y)) + 1e-9)

    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH)  # [40,T]
    # ìƒ˜í”Œ ë‹¨ìœ„ z-score (extract_mfcc.pyì™€ ë™ì¼)
    m = mfcc.mean(); s = mfcc.std() if mfcc.std() > 1e-9 else 1.0
    mfcc = (mfcc - m) / s

    # pad/trim to MAX_T (train.MFCCDatasetì™€ ë™ì¼)
    T = mfcc.shape[1]
    if T < MAX_T:
        pad = MAX_T - T
        mfcc = np.pad(mfcc, ((0,0),(0,pad)), mode='constant')
    else:
        mfcc = mfcc[:, :MAX_T]
    return mfcc  # [40,128]

# ====== ë§ˆì´í¬ ======
CHUNK = 1024
RECORD_SECONDS = 3

def record_audio(seconds=RECORD_SECONDS, rate=TARGET_SR):
    pa = pyaudio.PyAudio()
    stream = pa.open(format=pyaudio.paInt16, channels=1, rate=rate,
                     input=True, frames_per_buffer=CHUNK)
    print(f"ğŸ™ï¸ {seconds}ì´ˆ ë™ì•ˆ ë§í•´ë³´ì„¸ìš”...")
    frames = []
    for _ in range(int(rate/CHUNK*seconds)):
        data = stream.read(CHUNK, exception_on_overflow=False)
        frames.append(np.frombuffer(data, dtype=np.int16))
    stream.stop_stream(); stream.close(); pa.terminate()
    y = np.concatenate(frames).astype(np.float32) / 32768.0
    return y, rate

def main():
    # ëª¨ë¸ ë¡œë“œ
    model = CNN_GRU_Model(num_classes=len(EMOTIONS)).to(DEVICE).eval()
    state = torch.load(MODEL_PATH, map_location=DEVICE)  # ì €ì¥ì€ ìˆœìˆ˜ state_dict
    model.load_state_dict(state)

    # ë…¹ìŒ â†’ MFCC
    y, sr = record_audio()
    mfcc = make_mfcc(y, sr)                              # [40,128]
    x = torch.from_numpy(mfcc[None,None,:,:]).to(DEVICE) # [1,1,40,128]

    # ì¶”ë¡ 
    with torch.no_grad():
        logits = model(x)                                # [1,7]
        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]
    top = int(np.argmax(probs))
    print(f"ğŸ§  ê²°ê³¼: {EMOTIONS[top]} (p={probs[top]:.2f})")
    top3 = probs.argsort()[-3:][::-1]
    print("Top-3:", [(EMOTIONS[i], float(probs[i])) for i in top3])

if __name__ == "__main__":
    main()
